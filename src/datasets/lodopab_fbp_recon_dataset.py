from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset

from src.utils.paths import get_project_root, safe_join_rel

@dataclass(frozen=True)
class LoDoPaBFBPConfig:
    """
    Configuration for LoDoPaB FBP reconstruction dataset.

    Parameters
    ----------
    manifest_rel :
        Relative path (from project root) to FBP manifest CSV
        generated by reconstruct_fbp_lodopab.
        Example: "data/prepared/lodopab_fbp/train/fbp_manifest.csv".
    patch_size :
        Optional square patch size in pixels. If None, use full slice.
    seed :
        Random seed for patch sampling (if any).
    """

    manifest_rel: str = "data/prepared/lodopab_fbp/train/fbp_manifest.csv"
    patch_size: Optional[int] = None
    seed: int = 42

class LoDoPaBFBPDataset(Dataset):
    """
    LoDoPaB FBP dataset: FBP noisy image -> GT clean image.

    Each sample
    -----------
    Returns a dict:
        "input"  : torch.float32 [1, Hc, Wc] (FBP noisy image)
        "target" : torch.float32 [1, Hc, Wc] (GT image)
        "meta"   : dict with manifest fields + domain info.
    """

    def __init__(self, cfg: LoDoPaBFBPConfig) -> None:
        super().__init__()
        self.cfg = cfg

        root = get_project_root()
        manifest_path = root / cfg.manifest_rel
        self.manifest = pd.read_csv(manifest_path)
        required_cols = ["index", "path_fbp", "path_gt"]
        missing = set(required_cols) - set(self.manifest.columns)
        if missing:
            raise ValueError(f"[DATASET:FBP] Missing required columns: {missing}")

        self.rng = np.random.default_rng(seed=cfg.seed)

        self.root = root

    def __len__(self) -> int:
        return len(self.manifest)


    def _load_slice(self, path_rel: str) -> np.ndarray:
        """
        Load a 2D numpy slice from a relative .npy path.

        Parameters
        ----------
        path_rel :
            Relative path from project root.

        Returns
        -------
        np.ndarray
            2D float32 array, shape (H, W).
        """
        path_abs = safe_join_rel(path_rel, root=self.root)
        x = np.load(path_abs).astype(np.float32, copy=False)
        assert np.isfinite(x).all(), f"[DATASET:FBP] Non-finite values in this slice"
        return x

    def _random_crop_pair(self, x1: np.ndarray, x2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Optionally apply a random square crop of size `patch_size`.

        Phase 1: patch_size is None -> return full slice.
        """
        if self.cfg.patch_size is None:
            return x1, x2

        patch = int(self.cfg.patch_size)
        h, w = x1.shape
        if patch > min(h, w):
            raise ValueError(
                f"patch_size={patch} larger than image size {x1.shape}"
            )

        r0 = self.rng.integers(0, h - patch + 1)
        c0 = self.rng.integers(0, w - patch + 1)

        x1_crop = x1[r0: r0 + patch, c0: c0 + patch]
        x2_crop = x2[r0: r0 + patch, c0: c0 + patch]
        return x1_crop, x2_crop

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Build one LoDoPaB FBP training sample.

        Returns
        -------
        dict
            {"input": Tensor, "target": Tensor, "meta": dict}
        """

        row = self.manifest.iloc[idx]
        fbp_np = self._load_slice(row["path_fbp"])
        gt_np = self._load_slice(row["path_gt"])

        if self.cfg.patch_size is not None:
            fbp_np, gt_np = self._random_crop_pair(fbp_np, gt_np)

        fbp_t = torch.from_numpy(fbp_np).unsqueeze(0).float()
        gt_t = torch.from_numpy(gt_np).unsqueeze(0).float()

        meta: Dict[str, Any] = {
            "index": int(row["index"]),
            "path_fbp": row["path_fbp"],
            "path_gt": row["path_gt"],
            "split": row.get("split", "unknown"),
            "domain": "lodopab",
        }

        return {"input": fbp_t, "target": gt_t, "meta": meta}

