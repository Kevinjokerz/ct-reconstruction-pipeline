{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for CT Image Reconstruction\n",
    "## LoDoPaB Dataset - Training on Lambda Labs GPU\n",
    "\n",
    "This notebook trains a U-Net to enhance FBP reconstructions of low-dose CT images.\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "Sinogram → FBP → Noisy Image → U-Net → Enhanced Image\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:46.025599Z",
     "start_time": "2025-11-18T15:55:43.988406Z"
    }
   },
   "source": [
    "# Install required packages (run once)\n",
    "!pip install torch torchvision h5py scikit-image scipy tqdm matplotlib tensorboard numpy pandas"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (2.9.1)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: h5py in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (3.15.1)\r\n",
      "Requirement already satisfied: scikit-image in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (0.25.2)\r\n",
      "Requirement already satisfied: scipy in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (1.16.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: matplotlib in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (3.10.7)\r\n",
      "Requirement already satisfied: tensorboard in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (2.20.0)\r\n",
      "Requirement already satisfied: numpy in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (2.1.3)\r\n",
      "Requirement already satisfied: pandas in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: filelock in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\r\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from scikit-image) (2.37.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from scikit-image) (2025.10.16)\r\n",
      "Requirement already satisfied: packaging>=21 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from scikit-image) (25.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from scikit-image) (0.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\r\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (2.3.1)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (1.76.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (3.10)\r\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (6.33.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from tensorboard) (3.1.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/davidranamagar/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\r\n",
      "Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.24.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.112725Z",
     "start_time": "2025-11-18T15:55:46.029525Z"
    }
   },
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from skimage.transform import iradon\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.157972Z",
     "start_time": "2025-11-18T15:55:48.154890Z"
    }
   },
   "source": [
    "# ===========================\n",
    "# CONFIGURATION - EDIT THESE\n",
    "# ===========================\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"../data/prepared/lodopab\")   # Update this!\n",
    "\n",
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 16,          # Increase for GPU (Lambda Labs can handle 16-32)\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_workers': 8,          # Use multiple workers on Lambda Labs\n",
    "    'device': 'cuda',          # Use GPU\n",
    "    'pin_memory': True,        # Enable for GPU\n",
    "    'save_dir': '../data/results/checkpoints/unet',\n",
    "    'log_dir': '../data/results/logs/unet',\n",
    "    'use_all_train_files': False,  # Set True to use all training data\n",
    "    'num_train_files': 10,     # Number of training files to use (if use_all_train_files=False)\n",
    "    'num_val_files': 2,        # Number of validation files to use\n",
    "}\n",
    "\n",
    "# Loss weights\n",
    "LOSS_WEIGHTS = {\n",
    "    'mse': 1.0,\n",
    "    'ssim': 0.1,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  num_epochs: 50\n",
      "  batch_size: 16\n",
      "  learning_rate: 0.0001\n",
      "  num_workers: 8\n",
      "  device: cuda\n",
      "  pin_memory: True\n",
      "  save_dir: ../data/results/checkpoints/unet\n",
      "  log_dir: ../data/results/logs/unet\n",
      "  use_all_train_files: False\n",
      "  num_train_files: 10\n",
      "  num_val_files: 2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.200858Z",
     "start_time": "2025-11-18T15:55:48.194506Z"
    }
   },
   "source": [
    "class LoDoPaBDataset(Dataset):\n",
    "    def __init__(self, obs_files, gt_files, transform=None, use_fbp=True):\n",
    "        self.obs_files = obs_files\n",
    "        self.gt_files = gt_files\n",
    "        self.transform = transform\n",
    "        self.use_fbp = use_fbp\n",
    "        \n",
    "        # Calculate total number of samples\n",
    "        self.file_offsets = [0]\n",
    "        for obs_file in obs_files:\n",
    "            with h5py.File(obs_file, 'r') as f:\n",
    "                self.file_offsets.append(self.file_offsets[-1] + f['data'].shape[0])\n",
    "        \n",
    "        self.total_samples = self.file_offsets[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def _get_file_and_index(self, idx):\n",
    "        \"\"\"Convert global index to (file_index, local_index)\"\"\"\n",
    "        for i in range(len(self.file_offsets) - 1):\n",
    "            if idx < self.file_offsets[i + 1]:\n",
    "                file_idx = i\n",
    "                local_idx = idx - self.file_offsets[i]\n",
    "                return file_idx, local_idx\n",
    "        raise IndexError(f\"Index {idx} out of range\")\n",
    "    \n",
    "    def _simple_fbp(self, sinogram):\n",
    "        \"\"\"\n",
    "        FBP reconstruction using skimage\n",
    "        \n",
    "        LoDoPaB sinogram: (1000, 513) = (angles, detectors)\n",
    "        iradon expects: (detectors, angles)\n",
    "        \"\"\"\n",
    "        # Transpose: (1000, 513) → (513, 1000)\n",
    "        sinogram_transposed = sinogram.T\n",
    "        \n",
    "        # 1000 angles from 0 to 180 degrees\n",
    "        theta = np.linspace(0, 180, sinogram.shape[0], endpoint=False)\n",
    "        \n",
    "        # Perform FBP\n",
    "        reconstructed = iradon(\n",
    "            sinogram_transposed,\n",
    "            theta=theta,\n",
    "            filter_name='ramp',\n",
    "            interpolation='linear',\n",
    "            circle=False,\n",
    "            output_size= 362\n",
    "        )\n",
    "        \n",
    "        # Resize to 362×362\n",
    "        if reconstructed.shape != (362, 362):\n",
    "            scale_y = 362 / reconstructed.shape[0]\n",
    "            scale_x = 362 / reconstructed.shape[1]\n",
    "            reconstructed = zoom(reconstructed, (scale_y, scale_x), order=1)\n",
    "\n",
    "        reconstructed = np.rot90(reconstructed, k=-1)\n",
    "        return reconstructed.astype(np.float32)\n",
    "    \n",
    "    def _normalize(self, img):\n",
    "        \"\"\"Normalize image to [0, 1]\"\"\"\n",
    "        img_min = img.min()\n",
    "        img_max = img.max()\n",
    "        if img_max - img_min > 1e-8:\n",
    "            return (img - img_min) / (img_max - img_min)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, local_idx = self._get_file_and_index(idx)\n",
    "        \n",
    "        # Open files on-demand (multiprocessing-safe)\n",
    "        with h5py.File(self.obs_files[file_idx], 'r') as f_obs, \\\n",
    "             h5py.File(self.gt_files[file_idx], 'r') as f_gt:\n",
    "            \n",
    "            sinogram = f_obs['data'][local_idx].astype(np.float32)\n",
    "            ground_truth = f_gt['data'][local_idx].astype(np.float32)\n",
    "        \n",
    "        # Apply FBP reconstruction\n",
    "        if self.use_fbp:\n",
    "            input_img = self._simple_fbp(sinogram)\n",
    "        else:\n",
    "            # For testing: use ground truth with noise\n",
    "            input_img = ground_truth + np.random.normal(0, 0.05, ground_truth.shape).astype(np.float32)\n",
    "        \n",
    "        # Normalize\n",
    "        input_img = self._normalize(input_img)\n",
    "        ground_truth = self._normalize(ground_truth)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_img = torch.from_numpy(input_img).unsqueeze(0)  # (1, 362, 362)\n",
    "        target_img = torch.from_numpy(ground_truth).unsqueeze(0)\n",
    "        \n",
    "        return input_img, target_img"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. U-Net Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.232940Z",
     "start_time": "2025-11-18T15:55:48.225977Z"
    }
   },
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double Convolution block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle size mismatch\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                     diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # Concatenate skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"Output convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net for CT Image Reconstruction\"\"\"\n",
    "    def __init__(self, n_channels=1, n_classes=1, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        \n",
    "        # Output\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.305923Z",
     "start_time": "2025-11-18T15:55:48.302931Z"
    }
   },
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined MSE + SSIM loss\"\"\"\n",
    "    def __init__(self, mse_weight=1.0, ssim_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def ssim_loss(self, pred, target):\n",
    "        \"\"\"Simplified SSIM loss\"\"\"\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "        \n",
    "        mu_pred = pred.mean()\n",
    "        mu_target = target.mean()\n",
    "        \n",
    "        sigma_pred = pred.var()\n",
    "        sigma_target = target.var()\n",
    "        sigma_pred_target = ((pred - mu_pred) * (target - mu_target)).mean()\n",
    "        \n",
    "        ssim = ((2 * mu_pred * mu_target + C1) * (2 * sigma_pred_target + C2)) / \\\n",
    "               ((mu_pred ** 2 + mu_target ** 2 + C1) * (sigma_pred + sigma_target + C2))\n",
    "        \n",
    "        return 1 - ssim\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        ssim = self.ssim_loss(pred, target)\n",
    "        \n",
    "        total_loss = self.mse_weight * mse + self.ssim_weight * ssim\n",
    "        \n",
    "        return total_loss, {'mse': mse.item(), 'ssim': ssim.item()}"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.362249Z",
     "start_time": "2025-11-18T15:55:48.356565Z"
    }
   },
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mse = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Loss\n",
    "        loss, loss_dict = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_dict['mse']\n",
    "        total_ssim += loss_dict['ssim']\n",
    "        \n",
    "        # Update progress\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'mse': f\"{loss_dict['mse']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), total_mse / len(dataloader), total_ssim / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mse = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc='Validation'):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss, loss_dict = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mse += loss_dict['mse']\n",
    "            total_ssim += loss_dict['ssim']\n",
    "    \n",
    "    return total_loss / len(dataloader), total_mse / len(dataloader), total_ssim / len(dataloader)\n",
    "\n",
    "\n",
    "def save_sample_images(model, dataloader, device, save_dir, epoch):\n",
    "    \"\"\"Save sample reconstructions\"\"\"\n",
    "    model.eval()\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs, targets = next(iter(dataloader))\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Plot first 4 samples\n",
    "        fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "        for i in range(min(4, inputs.shape[0])):\n",
    "            axes[i, 0].imshow(inputs[i, 0].cpu().numpy(), cmap='gray')\n",
    "            axes[i, 0].set_title('Input (FBP)')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(outputs[i, 0].cpu().numpy(), cmap='gray')\n",
    "            axes[i, 1].set_title('Output (U-Net)')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(targets[i, 0].cpu().numpy(), cmap='gray')\n",
    "            axes[i, 2].set_title('Target (Ground Truth)')\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / f'epoch_{epoch:03d}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.396066Z",
     "start_time": "2025-11-18T15:55:48.385663Z"
    }
   },
   "source": [
    "# Find data files\n",
    "print(f\"Looking for data in: {DATA_DIR}\")\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"ERROR: Data directory not found: {DATA_DIR}\")\n",
    "    print(\"Please update DATA_DIR in the configuration cell!\")\n",
    "else:\n",
    "    # Get training files\n",
    "    train_obs = sorted(list(DATA_DIR.glob(\"observation_train_*.hdf5\")))\n",
    "    train_gt = sorted(list(DATA_DIR.glob(\"ground_truth_train_*.hdf5\")))\n",
    "    \n",
    "    # Get validation files\n",
    "    val_obs = sorted(list(DATA_DIR.glob(\"observation_validation_*.hdf5\")))\n",
    "    val_gt = sorted(list(DATA_DIR.glob(\"ground_truth_validation_*.hdf5\")))\n",
    "    \n",
    "    print(f\"\\nFound {len(train_obs)} training observation files\")\n",
    "    print(f\"Found {len(train_gt)} training ground truth files\")\n",
    "    print(f\"Found {len(val_obs)} validation observation files\")\n",
    "    print(f\"Found {len(val_gt)} validation ground truth files\")\n",
    "    \n",
    "    # Subset if needed\n",
    "    if not CONFIG['use_all_train_files']:\n",
    "        train_obs = train_obs[:CONFIG['num_train_files']]\n",
    "        train_gt = train_gt[:CONFIG['num_train_files']]\n",
    "        print(f\"\\nUsing {len(train_obs)} training files (subset)\")\n",
    "    \n",
    "    val_obs = val_obs[:CONFIG['num_val_files']]\n",
    "    val_gt = val_gt[:CONFIG['num_val_files']]\n",
    "    print(f\"Using {len(val_obs)} validation files\")\n",
    "    \n",
    "    if not train_obs or not train_gt or not val_obs or not val_gt:\n",
    "        print(\"\\nERROR: Missing data files!\")\n",
    "        print(\"Make sure your LoDoPaB data is in the correct directory.\")\n",
    "    else:\n",
    "        print(\"\\n✓ Data files found successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for data in: ../data/prepared/lodopab\n",
      "\n",
      "Found 280 training observation files\n",
      "Found 280 training ground truth files\n",
      "Found 28 validation observation files\n",
      "Found 28 validation ground truth files\n",
      "\n",
      "Using 10 training files (subset)\n",
      "Using 2 validation files\n",
      "\n",
      "✓ Data files found successfully!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:48.418540Z",
     "start_time": "2025-11-18T15:55:48.408931Z"
    }
   },
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = LoDoPaBDataset(train_obs, train_gt, use_fbp=True)\n",
    "val_dataset = LoDoPaBDataset(val_obs, val_gt, use_fbp=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders (optimized for GPU)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "print(f\"\\nBatches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Training samples: 1280\n",
      "Validation samples: 256\n",
      "\n",
      "Batches per epoch: 80\n",
      "Validation batches: 16\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:49.216181Z",
     "start_time": "2025-11-18T15:55:48.422631Z"
    }
   },
   "source": [
    "# Device\n",
    "device = torch.device(CONFIG['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model\n",
    "model = UNet(n_channels=1, n_classes=1, bilinear=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = CombinedLoss(\n",
    "    mse_weight=LOSS_WEIGHTS['mse'],\n",
    "    ssim_weight=LOSS_WEIGHTS['ssim']\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model initialized successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Model parameters: 17,261,825\n",
      "Trainable parameters: 17,261,825\n",
      "\n",
      "✓ Model initialized successfully!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:55:49.333619Z",
     "start_time": "2025-11-18T15:55:49.235731Z"
    }
   },
   "source": [
    "# Create directories\n",
    "save_dir = Path(CONFIG['save_dir'])\n",
    "log_dir = Path(CONFIG['log_dir'])\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TensorBoard\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(log_dir / f'run_{timestamp}')\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    train_loss, train_mse, train_ssim = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mse, val_ssim = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Log metrics\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('MSE/train', train_mse, epoch)\n",
    "    writer.add_scalar('MSE/val', val_mse, epoch)\n",
    "    writer.add_scalar('SSIM/train', train_ssim, epoch)\n",
    "    writer.add_scalar('SSIM/val', val_ssim, epoch)\n",
    "    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_mse'].append(train_mse)\n",
    "    history['val_mse'].append(val_mse)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, MSE: {train_mse:.4f}, SSIM: {train_ssim:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, MSE: {val_mse:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save sample images\n",
    "    if epoch % 5 == 0:\n",
    "        save_sample_images(model, val_loader, device, save_dir / 'samples', epoch)\n",
    "        print(f\"✓ Saved sample images\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, save_dir / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, save_dir / f'checkpoint_epoch_{epoch:03d}.pth')\n",
    "        print(f\"✓ Saved checkpoint\")\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Models saved to: {save_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m60\u001B[39m)\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, CONFIG[\u001B[33m'\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m'\u001B[39m] + \u001B[32m1\u001B[39m):\n\u001B[32m     20\u001B[39m     \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     train_loss, train_mse, train_ssim = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m     \u001B[38;5;66;03m# Validate\u001B[39;00m\n\u001B[32m     26\u001B[39m     val_loss, val_mse, val_ssim = validate(model, val_loader, criterion, device)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, dataloader, criterion, optimizer, device, epoch)\u001B[39m\n\u001B[32m      5\u001B[39m total_mse = \u001B[32m0\u001B[39m\n\u001B[32m      6\u001B[39m total_ssim = \u001B[32m0\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m pbar = \u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mEpoch \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (inputs, targets) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(pbar):\n\u001B[32m     10\u001B[39m     inputs = inputs.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages/tqdm/notebook.py:234\u001B[39m, in \u001B[36mtqdm_notebook.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    232\u001B[39m unit_scale = \u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m\n\u001B[32m    233\u001B[39m total = \u001B[38;5;28mself\u001B[39m.total * unit_scale \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m \u001B[38;5;28mself\u001B[39m.container = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstatus_printer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mncols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[38;5;28mself\u001B[39m.container.pbar = proxy(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    236\u001B[39m \u001B[38;5;28mself\u001B[39m.displayed = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Project/COSC /4372 Fundametals of Medical Imaging/ct-reconstruction-pipeline/.venv/lib/python3.12/site-packages/tqdm/notebook.py:108\u001B[39m, in \u001B[36mtqdm_notebook.status_printer\u001B[39m\u001B[34m(_, total, desc, ncols)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;66;03m# Fallback to text bar if there's no total\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# if not total:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    105\u001B[39m \n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# Prepare IPython progress bar\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# #187 #451 #558 #872\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(WARN_NOIPYW)\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m total:\n\u001B[32m    110\u001B[39m     pbar = IProgress(\u001B[38;5;28mmin\u001B[39m=\u001B[32m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m=total)\n",
      "\u001B[31mImportError\u001B[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# MSE\n",
    "ax2.plot(history['train_mse'], label='Train MSE')\n",
    "ax2.plot(history['val_mse'], label='Val MSE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE')\n",
    "ax2.set_title('Training and Validation MSE')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved training history plot to: {save_dir / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Best Model and Test"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(save_dir / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Test on validation set\n",
    "with torch.no_grad():\n",
    "    inputs, targets = next(iter(val_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    \n",
    "    for i in range(min(4, inputs.shape[0])):\n",
    "        # Input (FBP)\n",
    "        axes[0, i].imshow(inputs[i, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Sample {i+1}\\nInput (FBP)')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Output (U-Net)\n",
    "        axes[1, i].imshow(outputs[i, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[1, i].set_title('Output (U-Net)')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Target (Ground Truth)\n",
    "        axes[2, i].imshow(targets[i, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[2, i].set_title('Ground Truth')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'final_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved final results to: {save_dir / 'final_results.png'}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
